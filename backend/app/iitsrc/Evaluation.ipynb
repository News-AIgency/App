{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e01903f-4bd6-4c3f-bafe-bc1ac0ce4701",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from bert_score import score\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589bb7dd-7f13-447b-8fdd-1fe2d0ab5568",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f66d47ef-3f18-4f91-9c86-5654e30be4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_articles = pd.read_csv('original_articles.csv', delimiter=';')\n",
    "generated_articles = pd.read_csv('generated_articles.csv', delimiter=';')\n",
    "llm_as_judge_scores = pd.read_csv('llm_as_judge_scores.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca986ca-ee2c-4fb6-9009-759b701c3919",
   "metadata": {},
   "source": [
    "Calculate bert, rouge1, rouge2, rougeL score for each article and save each value in dictionary for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb4a8e2-c473-4590-87e9-6102ef2715b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "sections = ['Headline', 'Perex', 'Article']\n",
    "\n",
    "# Dictionary for scores for each article\n",
    "all_scores = {section: {'bert': [], 'rouge1': [], 'rouge2': [], 'rougeL': [], 'llm_as_judge': []} for section in sections}\n",
    "# Calculate and save the scores\n",
    "for section in sections:\n",
    "    references = original_articles[section].dropna().tolist()\n",
    "    hypotheses = generated_articles[section].dropna().tolist()\n",
    "\n",
    "    # Compute BERTScore\n",
    "    P, R, F1 = score(hypotheses, references, lang=\"en\", rescale_with_baseline=False)\n",
    "    all_scores[section]['bert'] = F1.tolist()\n",
    "\n",
    "    # Compute ROUGE scores\n",
    "    rouge_scores = [scorer.score(h, r) for h, r in zip(hypotheses, references)]\n",
    "    all_scores[section]['rouge1'] = [r['rouge1'].fmeasure for r in rouge_scores]\n",
    "    all_scores[section]['rouge2'] = [r['rouge2'].fmeasure for r in rouge_scores]\n",
    "    all_scores[section]['rougeL'] = [r['rougeL'].fmeasure for r in rouge_scores]\n",
    "\n",
    "all_scores['Headline']['llm_as_judge'] = llm_as_judge_scores['Headline'].tolist()\n",
    "all_scores['Perex']['llm_as_judge'] = llm_as_judge_scores['Perex'].tolist()\n",
    "all_scores['Article']['llm_as_judge'] = llm_as_judge_scores['Article'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846bbd3e-521b-49c4-8d7b-6734a82064ea",
   "metadata": {},
   "source": [
    "# Mean scores for sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7073a3c8-dc44-41f9-9ee1-575847588378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean scores\n",
    "results = {\n",
    "    'Section': [], 'Mean BERTScore': [], 'Mean ROUGE-1': [], 'Mean ROUGE-2': [], 'Mean ROUGE-L': [], 'Mean LLM as judge': [],\n",
    "}\n",
    "for section in sections:\n",
    "    results['Section'].append(section)\n",
    "    results['Mean BERTScore'].append(np.mean(all_scores[section]['bert']))\n",
    "    results['Mean ROUGE-1'].append(np.mean(all_scores[section]['rouge1']))\n",
    "    results['Mean ROUGE-2'].append(np.mean(all_scores[section]['rouge2']))\n",
    "    results['Mean ROUGE-L'].append(np.mean(all_scores[section]['rougeL']))\n",
    "    results['Mean LLM as judge'].append(np.mean(all_scores[section]['llm_as_judge']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d2d59e-a0b1-47b9-a9e4-0697d8ed740c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac223dc7-3ef2-4878-8828-2f87e9f28869",
   "metadata": {},
   "source": [
    "# Calculate tag accuracy\n",
    "This metric does not completely reflect the reality of correctly generated tags. For example, generated tags are missing spaces between words as well as diacritic. You can also see that sometimes it generates a synonym tags compared to the original or adds additional word into the tag. In these scenarios the tags have the same meaning, but will be evaluated as incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48740489-436f-458c-abc3-633c16b7805f",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_rows = original_articles[\"Tags\"].notna() & (original_articles[\"Tags\"] != \"N/A\")\n",
    "\n",
    "# Now, filter both references (original_articles) and hypotheses (generated_articles) based on valid rows\n",
    "references = original_articles[\"Tags\"][valid_rows]\n",
    "hypotheses = generated_articles[\"Tags\"][valid_rows]\n",
    "\n",
    "correct_count = 0\n",
    "total_count = 0\n",
    "\n",
    "for ref, hyp in zip(references, hypotheses):\n",
    "    # Transform references to lowercase and split by comma\n",
    "    ref_tags = {tag.strip().lower() for tag in ref.split(',')}\n",
    "    # Split hypotheses by space and convert to set\n",
    "    hyp_tags = {tag.strip().lower() for tag in hyp.split()}\n",
    "\n",
    "    print(f\"ref: {ref_tags}\\nhyp: {hyp_tags}\")\n",
    "\n",
    "    # For each tag in the hypothesis, check if it exists in the references\n",
    "    for tag in hyp_tags:\n",
    "        if tag in ref_tags:\n",
    "            correct_count += 1\n",
    "        total_count += 1\n",
    "\n",
    "# Calculate the tag accuracy\n",
    "tag_accuracy = correct_count / total_count if total_count > 0 else 0\n",
    "print(f\"\\nTag accuracy metric: {tag_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7012733-8972-43b9-a1cb-2cc96e4b984f",
   "metadata": {},
   "source": [
    "# Plotting the score distribution of the different score types across section through histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e275b39c-33d4-47c7-b438-075f7a552c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "for i, section in enumerate(sections):\n",
    "    plt.subplot(3, 1, i+1)\n",
    "    plt.hist(all_scores[section]['bert'], bins=20, alpha=0.7, label='BERTScore', color='blue')\n",
    "    plt.hist(all_scores[section]['rouge1'], bins=20, alpha=0.7, label='ROUGE-1', color='green')\n",
    "    plt.hist(all_scores[section]['rouge2'], bins=20, alpha=0.7, label='ROUGE-2', color='red')\n",
    "    plt.hist(all_scores[section]['rougeL'], bins=20, alpha=0.7, label='ROUGE-L', color='purple')\n",
    "    plt.hist(all_scores[section]['llm_as_judge'], bins=20, alpha=0.7, label='LLM as judge', color='yellow')\n",
    "    plt.title(f'Score Distribution for {section}')\n",
    "    plt.xlabel('Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d857af-b61c-4c9a-afe1-fb7883436ddd",
   "metadata": {},
   "source": [
    "# Boxplots of score types to showcase comparison of how well we did in different sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ae29b4-56fd-47b7-a74c-17056dce0cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "for i, metric in enumerate(['bert', 'rouge1', 'rouge2', 'rougeL', 'llm_as_judge']):\n",
    "    plt.subplot(3, 2, i+1)\n",
    "    plt.boxplot([all_scores[section][metric] for section in sections], tick_labels=sections)\n",
    "    plt.title(f'Boxplot of {metric.upper()} Scores')\n",
    "    plt.ylabel('Score')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72352edf-1ff8-496b-a2a9-f5664ad4157d",
   "metadata": {},
   "source": [
    "# Aggregated scores for each news site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b5fb89-6222-49b7-8e52-bbdc06b18a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_articles = pd.concat([original_articles, generated_articles], ignore_index=True)\n",
    "news_site_results = concat_articles.groupby('News Site').apply(lambda df: pd.Series({\n",
    "    'Mean_BERTScore_Headline': np.mean([all_scores['Headline']['bert'][i] for i in df.index if i < len(all_scores['Headline']['bert'])]),\n",
    "    'Mean_ROUGE_1_Headline': np.mean([all_scores['Headline']['rouge1'][i] for i in df.index if i < len(all_scores['Headline']['rouge1'])]),\n",
    "    'Mean_ROUGE_2_Headline': np.mean([all_scores['Headline']['rouge2'][i] for i in df.index if i < len(all_scores['Headline']['rouge2'])]),\n",
    "    'Mean_ROUGE_L_Headline': np.mean([all_scores['Headline']['rougeL'][i] for i in df.index if i < len(all_scores['Headline']['rougeL'])]),\n",
    "    'Mean_LLM_as_judge_Headline': np.mean([all_scores['Headline']['llm_as_judge'][i] for i in df.index if i < len(all_scores['Headline']['llm_as_judge'])]),\n",
    "    'Mean_BERTScore_Perex': np.mean([all_scores['Perex']['bert'][i] for i in df.index if i < len(all_scores['Perex']['bert'])]),\n",
    "    'Mean_ROUGE_1_Perex': np.mean([all_scores['Perex']['rouge1'][i] for i in df.index if i < len(all_scores['Perex']['rouge1'])]),\n",
    "    'Mean_ROUGE_2_Perex': np.mean([all_scores['Perex']['rouge2'][i] for i in df.index if i < len(all_scores['Perex']['rouge2'])]),\n",
    "    'Mean_ROUGE_L_Perex': np.mean([all_scores['Perex']['rougeL'][i] for i in df.index if i < len(all_scores['Perex']['rougeL'])]),\n",
    "    'Mean_LLM_as_judge_Perex': np.mean([all_scores['Perex']['llm_as_judge'][i] for i in df.index if i < len(all_scores['Perex']['llm_as_judge'])]),\n",
    "    'Mean_BERTScore_Article': np.mean([all_scores['Article']['bert'][i] for i in df.index if i < len(all_scores['Article']['bert'])]),\n",
    "    'Mean_ROUGE_1_Article': np.mean([all_scores['Article']['rouge1'][i] for i in df.index if i < len(all_scores['Article']['rouge1'])]),\n",
    "    'Mean_ROUGE_2_Article': np.mean([all_scores['Article']['rouge2'][i] for i in df.index if i < len(all_scores['Article']['rouge2'])]),\n",
    "    'Mean_ROUGE_L_Article': np.mean([all_scores['Article']['rougeL'][i] for i in df.index if i < len(all_scores['Article']['rougeL'])]),\n",
    "    'Mean_LLM_as_judge_Article': np.mean([all_scores['Article']['llm_as_judge'][i] for i in df.index if i < len(all_scores['Article']['llm_as_judge'])]),\n",
    "})).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8f52c1-6968-4ad8-be9f-34726db55955",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_site_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f8b73b-3e51-4a0b-accf-11b8648b2d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_site_results_transposed = news_site_results.set_index('News Site').T\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(news_site_results_transposed, annot=True, cmap='coolwarm', fmt='.4f', linewidths=0.5)\n",
    "plt.title('Heatmap of News Site Metrics')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0bf78f-b953-465b-860b-775a1c8da225",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
